{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nFrom the command line you can convert a notebook to python with this command:\\n\\nipython nbconvert --to python <YourNotebook>.ipynb\\n\\nYou may have to install the python mistune package:\\n\\nsudo pip install mistune\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os \n",
    "import random\n",
    "from random import randint\n",
    "import numpy as np\n",
    "import cv2\n",
    "sys.path.append('..')\n",
    "import Video_Utils\n",
    "import CNN_Utils\n",
    "import h5py\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Convolution2D, UpSampling2D, MaxPooling2D, ZeroPadding2D, Reshape, merge\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "from keras.layers.core import Lambda, Merge\n",
    "from keras.engine import Layer\n",
    "\n",
    "'''\n",
    "From the command line you can convert a notebook to python with this command:\n",
    "\n",
    "ipython nbconvert --to python <YourNotebook>.ipynb\n",
    "\n",
    "You may have to install the python mistune package:\n",
    "\n",
    "sudo pip install mistune\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################## GLOBAL VARIABLES #########################################\n",
    "# These variables are specific to the dataset of the problem. \n",
    "# Our data is the following: \n",
    "#\n",
    "# time-series of 4 cameras (128 x 128 x 3) looking at a robot complete a task (pick up object, place in bin)\n",
    "# + corresponding time series of 7-d vector of joint commands (6 DOF + gripper) denoting the position delta command.\n",
    "#\n",
    "# In future comments, we will refer to the states and actions as x(t) and u(t) respectively, where 'state' \n",
    "# denotes the visual data sensed from the world (the 4 cameras) and action is the 7-d joint position delta vector. \n",
    "#\n",
    "# Data was generated and collected using the V-REP (http://www.coppeliarobotics.com/) robot simulator \n",
    "# (free/education version). Please contact the author of this notebook for the specific scene and data-generation\n",
    "# script \n",
    "\n",
    "global CAM_W\n",
    "global CAM_H\n",
    "global CAM_C\n",
    "global NUM_CAMS\n",
    "global ACTION_LEN\n",
    "\n",
    "CAM_W = 128\n",
    "CAM_H = 128\n",
    "CAM_C = 3\n",
    "NUM_CAMS = 4\n",
    "ACTION_LEN = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################################### MODEL PARAMETERS #########################################\n",
    "global NUM_FUTURE_FRAMES\n",
    "global NUM_PAST_FRAMES\n",
    "#global NUM_VGG_FEAT_MAPS\n",
    "#global VGG_FEAT_W\n",
    "#global VGG_FEAT_H \n",
    "global GENC_1_FEAT_MAPS\n",
    "global GENC_2_FEAT_MAPS\n",
    "global GENC_3_FEAT_MAPS\n",
    "global GENC_4_FEAT_MAPS\n",
    "\n",
    "global GDEC_1_FEAT_MAPS\n",
    "global GDEC_2_FEAT_MAPS\n",
    "global GDEC_3_FEAT_MAPS\n",
    "global GDEC_4_FEAT_MAPS\n",
    "\n",
    "global CONVG5_FEAT_MAPS\n",
    "global CONVG4_FEAT_MAPS\n",
    "global CONVG3_FEAT_MAPS\n",
    "global CONVG2_FEAT_MAPS\n",
    "global CONVG1_FEAT_MAPS\n",
    "global G_DENSE1\n",
    "global G_DENSE2\n",
    "global G_DENSE3\n",
    "global GENERATOR_MODEL\n",
    "global DISCRIMINATOR_MODEL\n",
    "\n",
    "#VGG_FEAT_W = 64        # using \"block1_pool\"\n",
    "#VGG_FEAT_H = 64        # using \"block1_pool\"  \n",
    "#NUM_VGG_FEAT_MAPS = 64 # using \"block1_pool\" (per camera)\n",
    "NUM_FUTURE_FRAMES = 10\n",
    "NUM_PAST_FRAMES = 10\n",
    "# Generator layers \n",
    "    # Part 1: Encoding for current frame\n",
    "GENC_1_FEAT_MAPS = 64\n",
    "GENC_2_FEAT_MAPS = 32\n",
    "GENC_3_FEAT_MAPS = 16\n",
    "GENC_4_FEAT_MAPS = 8\n",
    "GDEC_4_FEAT_MAPS = CAM_C*NUM_CAMS\n",
    "GDEC_3_FEAT_MAPS = round(GDEC_4_FEAT_MAPS * 2)\n",
    "GDEC_2_FEAT_MAPS = round(GDEC_3_FEAT_MAPS * 2)\n",
    "GDEC_1_FEAT_MAPS = round(GDEC_2_FEAT_MAPS * 2)\n",
    "    # Part 2: DCGAN-type architecture\n",
    "CONVG4_FEAT_MAPS = CAM_C*NUM_CAMS*NUM_FUTURE_FRAMES    # e.g. == 3*4*10 = 120\n",
    "CONVG3_FEAT_MAPS = round(CONVG5_FEAT_MAPS * 2)       # e.g. == 240\n",
    "CONVG2_FEAT_MAPS = round(CONVG4_FEAT_MAPS * 2)       # e.g. == 480\n",
    "CONVG1_FEAT_MAPS = round(CONVG3_FEAT_MAPS * 2)       # e.g. == 960\n",
    " \n",
    "G_DENSE1 = 1200 # the vector going into this dense layer will be on order of 512+63 ~ 600-dim \n",
    "G_DENSE2 = 4096\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2906bf500eda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[1;31m# Model input #1: past frames\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0minput_prev_frames_raw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCAM_W\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCAM_H\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCAM_C\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mNUM_CAMS\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mNUM_PAST_FRAMES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'input_prev_frames_raw'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[1;31m# below: some layers to learn what information is important from the past\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0minput_prev_frames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConvolution2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCONVP1_FEAT_MAPS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'elu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mborder_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'same'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_prev_frames_raw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Input' is not defined"
     ]
    }
   ],
   "source": [
    "################################################# THE MODEL ####################################################\n",
    "'''\n",
    "An explanation of the dynamics model and the buffers: \n",
    "\n",
    "We are training a model to produce p(x(t+1: t+F) | x(t), u(t: t+F-1)) \n",
    "\n",
    "I.e.: predict the future F frames, \n",
    "      given the current frame, future F-1 actions to take\n",
    "      \n",
    "The GENERATOR_MODEL will be responsible for producing a tensor of shape: (F, W, H, 3*NUM_CAMS)\n",
    "\n",
    "The DISCRIMINATOR_MODEL will output a single scalar (e.g. in (-1, 1) range) after taking in the above ^ shape \n",
    "as input, + the proposed actions, and determine whether the frame-action sequence was real or generated. \n",
    "\n",
    "I.e.: Discriminator estimates p(sequence_real | x(t), x(t+1: t+F), u(t: t+F-1))\n",
    "\n",
    "'''\n",
    "# Can use part of pre-trained VGG model to seed features with reasonable features: (used online during training)  \n",
    "#vgg_preprocessor = CNN_Utils.GetVGGModel(\"block1_pool\", CAM_W, CAM_H, print_timing=1) \n",
    "# ^ Note: not using for this experiment (creates too many feature maps per camera for current hardware)\n",
    "\n",
    "# Gen Model input #1: current frame\n",
    "input_curr_frame = Input(shape=(CAM_W, CAM_H, CAM_C*NUM_CAMS), name='input_curr_frame')\n",
    "# Step 1: Compress and encode to relevant vectorized information -\n",
    "    # Idea: try regularizing by making and training a network branch that must be able \n",
    "    # to reconstruct the input frame from the encoding\n",
    "    # input_curr_frame sample shape: 128x128x12 (3channels * 4cameras)\n",
    "frame_enc = Convolution2D(GENC_1_FEAT_MAPS, 9, 9, strides=(2, 2), activation='elu', border_mode='same', name='genc_1')(input_curr_frame)\n",
    "frame_enc = MaxPooling2D((2, 2), border_mode='same')(frame_enc) #e.g. 64x64x64 (W,H,filters)\n",
    "frame_enc = Convolution2D(GENC_2_FEAT_MAPS, 7, 7, activation='elu', border_mode='same', name='genc_2')(frame_enc)\n",
    "frame_enc = MaxPooling2D((2, 2), border_mode='same')(frame_enc) #e.g. 32x32x32\n",
    "frame_enc = Convolution2D(GENC_3_FEAT_MAPS, 5, 5, activation='elu', border_mode='same', name='genc_3')(frame_enc)\n",
    "frame_enc = MaxPooling2D((2, 2), border_mode='same')(frame_enc) #e.g. 16x16x16\n",
    "frame_enc = Convolution2D(GENC_4_FEAT_MAPS, 3, 3, activation='elu', border_mode='same', name='genc_4')(frame_enc)\n",
    "frame_enc = MaxPooling2D((2, 2), border_mode='same')(frame_enc) #e.g. 8x8x8\n",
    "frame_enc_flat = Flatten()(frame_enc_flat, name='genc_flat') #e.g. 512-d vector --> for Generator usage\n",
    "    # Decoder-as-a-regularizer: \n",
    "frame_dec = Convolution2D(GDEC_1_FEAT_MAPS, 3, 3, activation='elu', border_mode='same', name='gdec_1')(frame_enc)\n",
    "frame_dec = UpSampling2D(size=(2, 2))(frame_dec) # e.g. 16x16x96\n",
    "frame_dec = Convolution2D(GDEC_2_FEAT_MAPS, 5, 5, activation='elu', border_mode='same', name='gdec_2')(frame_dec)\n",
    "frame_dec = UpSampling2D(size=(2, 2))(frame_dec) # e.g. 32x32x48\n",
    "frame_dec = Convolution2D(GDEC_3_FEAT_MAPS, 5, 5, activation='elu', border_mode='same', name='gdec_2')(frame_dec)\n",
    "frame_dec = UpSampling2D(size=(2, 2))(frame_dec) # e.g. 64x64x24\n",
    "frame_dec = Convolution2D(GDEC_4_FEAT_MAPS, 5, 5, activation='elu', border_mode='same', name='gdec_2')(frame_dec)\n",
    "frame_dec = UpSampling2D(size=(2, 2))(frame_dec) # e.g. 128x128x12\n",
    "frame_dec = Convolution2D(GDEC_4_FEAT_MAPS, 3, 3, activation='elu', border_mode='same', name='gdec_2')(frame_dec)\n",
    "# Gen Model input #2: future actions\n",
    "input_future_actions_raw = Input(shape=((NUM_FUTURE_FRAMES-1)*ACTION_LEN,), name='input_future_actions_raw')\n",
    "# Step 2: Merge inputs, project, reshape\n",
    "gen_merged_inputs = merge([frame_enc_flat, input_future_actions_raw], mode='concat', concat_axis=1, name='merged_gen_inputs')\n",
    "G_A1 = Dense(output_dim=G_DENSE1, activation='elu', name='G_A1')(gen_merged_inputs)\n",
    "G_A2 = Dense(output_dim=G_DENSE2, activation='elu', name='G_A2')(G_A1)\n",
    "\n",
    "gen_out = Reshape((4, 4, round(G_DENSE2/(4*4)), name='gout_1')(G_A2) # e.g. 4x4x256\n",
    "gen_out = UpSampling2D(size=(2, 2))(gen_out) # e.g. 8x8x256\n",
    "gen_out = Convolution2D(CONVG1_FEAT_MAPS, 3, 3, activation='elu', border_mode='same', name='gout_2')(gen_out)\n",
    "gen_out = UpSampling2D(size=(2, 2))(gen_out) # e.g. 16x16x960 \n",
    "gen_out = Convolution2D(CONVG2_FEAT_MAPS, 5, 5, activation='elu', border_mode='same', name='gout_3')(gen_out)\n",
    "gen_out = UpSampling2D(size=(2, 2))(gen_out) # e.g. 32x32x480\n",
    "gen_out = Convolution2D(CONVG3_FEAT_MAPS, 5, 5, activation='elu', border_mode='same', name='gout_4')(gen_out)\n",
    "gen_out = UpSampling2D(size=(2, 2))(gen_out) # e.g. 64x64x240\n",
    "gen_out = Convolution2D(CONVG4_FEAT_MAPS, 7, 7, activation='elu', border_mode='same', name='gout_5')(gen_out)\n",
    "gen_out = UpSampling2D(size=(2, 2))(gen_out) # e.g. 128x128x120\n",
    "gen_out = Convolution2D(CONVG4_FEAT_MAPS, 3, 3, activation='elu', border_mode='same', name='gout_final')(gen_out)\n",
    "                  \n",
    "                  \n",
    "# Define the full model structure: \n",
    "gen_model_inputs = [input_prev_frames_raw, input_prev_actions_raw, input_future_actions_raw]\n",
    "# Final output shape: W, H, 3*NUM_CAMS*NUM_FUTURE_FRAMES, e.g. 128x128x120\n",
    "gen_model_outputs = [gen_out] \n",
    "\n",
    "\n",
    "# Now merge with information about future actions: \n",
    "merged_input = merge([merged_input, future_action_branch], \n",
    "                      mode='concat', concat_axis=3, name='merged_efficient_inputs_all')\n",
    "# ^ This will have size [W, H, NUM_PAST_FRAMES + 1], e.g. [128, 128, 11]\n",
    "\n",
    "# Conv layers for predicting the next frames given all relevant data: \n",
    "Out_C1 = Convolution2D(CONVF1_FEAT_MAPS, 7, 7, activation='elu', border_mode='same', name='Out_C1')(merged_input)\n",
    "Out_C2 = Convolution2D(CONVF2_FEAT_MAPS, 3, 3, activation='elu', border_mode='same', name='Out_C2')(Out_C1)\n",
    "Out_C3 = Convolution2D(CONVF3_FEAT_MAPS, 3, 3, activation='elu', border_mode='same', name='Out_C3')(Out_C2)\n",
    "Out_C4 = Convolution2D(CONVF4_FEAT_MAPS, 3, 3, activation='elu', border_mode='same', name='Out_C4')(Out_C3)\n",
    "# ^ Out_C4 is a final output layer, spits out a frame (defined as WxHx3*num_cams*num_future_frames)\n",
    "\n",
    "# Define the full model structure: \n",
    "model_inputs = [input_prev_frames_raw, input_prev_actions_raw, input_future_actions_raw]\n",
    "# Review of input dimensions: (caller of train/predict must expand_dims to achieve shapes)\n",
    "# input_prev_frames_raw: [W, H, 3*NUM_CAMS*NUM_PAST_FRAMES] e.g. [1, 128, 128, 120]\n",
    "# input_prev_actions_raw: [NUM_PAST_FRAMES*ACTION_LEN,] e.g. [1, 70] \n",
    "# input_future_actions_raw: [(NUM_FUTURE_FRAMES-1)*ACTION_LEN,] e.g. [1, 63]\n",
    "model_outputs = [Out_C4] \n",
    "# Target output data: \n",
    "# Out_C4: [W, H, 3*NUM_CAMS*NUM_FUTURE_FRAMES], e.g. [1, 128, 128, 120]\n",
    "UDM_model = Model(input=model_inputs, output=model_outputs)\n",
    "\n",
    "UDM_optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "UDM_model.compile(loss='binary_crossentropy', optimizer=UDM_optimizer) \n",
    "# ^ TODO: custom loss function more appropriate for sequence of similar images...   \n",
    "\n",
    "UDM_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################################### INITIALIZATION, SETUP OF MODEL #########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########################################## Globals for training params ####################################\n",
    "\n",
    "global TOTAL_TRAINING_ITRS\n",
    "global SAVE_CHECKPOINT_ITRS\n",
    "global NUM_DEMONSTRATIONS\n",
    "global CURR_DEMONSTRATION\n",
    "global LENGTH_CURR_DEMONSTRATION # e.g. current demo we're looking at is 230 timesteps\n",
    "global T_CURR_DEMONSTRATION      # and e.g. we're currently on timestep 87\n",
    "global PERCENT_TRAIN             # percent of data used for training vs. valudation\n",
    "global DEMONSTRATION_FOLDERS\n",
    "global TRAINING_FOLDERS\n",
    "global TESTING_FOLDERS\n",
    "global NUM_TRAINING_DEMONSTRATIONS\n",
    "global NUM_TESTING_DEMONSTRATIONS\n",
    "global IMAGE_COMPARE_CHECKPOINT\n",
    "\n",
    "global PREV_FRAMES_BUFFER\n",
    "global PREV_ACTION_BUFFER\n",
    "global FUTURE_FRAMES_BUFFER\n",
    "global FUTURE_ACTION_BUFFER\n",
    "global MODEL_LOSS_BUFFER\n",
    "\n",
    "PREV_FRAMES_BUFFER = np.zeros((NUM_PAST_FRAMES, CAM_W, CAM_H, CAM_C))\n",
    "PREV_ACTION_BUFFER = np.zeros((NUM_PAST_FRAMES, ACTION_LEN))\n",
    "FUTURE_FRAMES_BUFFER = np.zeros((NUM_FUTURE_FRAMES, CAM_W, CAM_H, CAM_C))\n",
    "FUTURE_ACTION_BUFFER = np.zeros((NUM_FUTURE_FRAMES-1, ACTION_LEN))\n",
    "\n",
    "TOTAL_TRAINING_ITRS = 100000\n",
    "SAVE_CHECKPOINT_ITRS = 100\n",
    "IMAGE_COMPARE_CHECKPOINT = 5\n",
    "CURR_DEMONSTRATION = -1 # start on the first one, loop to another one each itr\n",
    "LENGTH_CURR_DEMONSTRATION = -1\n",
    "PERCENT_TRAIN = 0.75\n",
    "MODEL_LOSS_BUFFER = np.zeros((10,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running program...\n",
      "u_sequence_1486408000621\n",
      "u_sequence_1486420877768\n",
      "u_sequence_1486421372155\n",
      "u_sequence_1486422003939\n",
      "u_sequence_1486423780289\n",
      "u_sequence_1486424226636\n",
      "u_sequence_1486424664609\n",
      "u_sequence_1486425113072\n",
      "u_sequence_1486425723665\n",
      "u_sequence_1486426394928\n",
      "u_sequence_1486483126414\n",
      "u_sequence_1486483372904\n",
      "u_sequence_1486483607589\n",
      "u_sequence_1486483842933\n",
      "u_sequence_1486484146973\n",
      "u_sequence_1486484663068\n",
      "u_sequence_1486485509467\n",
      "u_sequence_1486485832231\n",
      "u_sequence_1486486076841\n",
      "u_sequence_1486486803219\n",
      "15\n",
      "5\n",
      "20\n",
      "(185, 128, 128, 12)\n",
      "(185, 7)\n",
      "\n",
      "===== Training on robot sample run #:13 with num timesteps: 185\n",
      "\n",
      "\n",
      "Loaded data for timestep 8 in 24.032 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 0 / timestep: 8\n",
      "Model update time: 15.453 secs\n",
      "Current Model Loss: -0.436337\n",
      "[[-0.43633705]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n",
      "\n",
      "\n",
      "Image compare checkpoint...\n",
      "Model prediction time: 1.566 secs for 10 future frames.\n",
      "Wrote new image sample checkpoint at: sample_14905731885338917.png\n",
      "Model save checkpoint, itr: 0\n",
      "Model save time: 7.873 secs\n",
      "\n",
      "Loaded data for timestep 95 in 4.007 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 1 / timestep: 95\n",
      "Model update time: 5.365 secs\n",
      "Current Model Loss: -0.318998\n",
      "[[-0.31899783]\n",
      " [-0.43633705]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 80 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 2 / timestep: 80\n",
      "Model update time: 5.894 secs\n",
      "Current Model Loss: 1.28326\n",
      "[[ 1.28326082]\n",
      " [-0.31899783]\n",
      " [-0.43633705]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 73 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 3 / timestep: 73\n",
      "Model update time: 5.186 secs\n",
      "Current Model Loss: 1.58438\n",
      "[[ 1.58437967]\n",
      " [ 1.28326082]\n",
      " [-0.31899783]\n",
      " [-0.43633705]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 74 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 4 / timestep: 74\n",
      "Model update time: 5.263 secs\n",
      "Current Model Loss: 1.04726\n",
      "[[ 1.04726195]\n",
      " [ 1.58437967]\n",
      " [ 1.28326082]\n",
      " [-0.31899783]\n",
      " [-0.43633705]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 111 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 5 / timestep: 111\n",
      "Model update time: 5.748 secs\n",
      "Current Model Loss: -0.267614\n",
      "[[-0.26761389]\n",
      " [ 1.04726195]\n",
      " [ 1.58437967]\n",
      " [ 1.28326082]\n",
      " [-0.31899783]\n",
      " [-0.43633705]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n",
      "\n",
      "\n",
      "Image compare checkpoint...\n",
      "Model prediction time: 1.088 secs for 10 future frames.\n",
      "Wrote new image sample checkpoint at: sample_14905732261980016.png\n",
      "\n",
      "Loaded data for timestep 32 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 6 / timestep: 32\n",
      "Model update time: 6.212 secs\n",
      "Current Model Loss: -0.0337407\n",
      "[[-0.03374073]\n",
      " [-0.26761389]\n",
      " [ 1.04726195]\n",
      " [ 1.58437967]\n",
      " [ 1.28326082]\n",
      " [-0.31899783]\n",
      " [-0.43633705]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 129 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 7 / timestep: 129\n",
      "Model update time: 5.699 secs\n",
      "Current Model Loss: -0.363772\n",
      "[[-0.36377195]\n",
      " [-0.03374073]\n",
      " [-0.26761389]\n",
      " [ 1.04726195]\n",
      " [ 1.58437967]\n",
      " [ 1.28326082]\n",
      " [-0.31899783]\n",
      " [-0.43633705]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 93 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 8 / timestep: 93\n",
      "Model update time: 5.671 secs\n",
      "Current Model Loss: -0.983543\n",
      "[[-0.98354292]\n",
      " [-0.36377195]\n",
      " [-0.03374073]\n",
      " [-0.26761389]\n",
      " [ 1.04726195]\n",
      " [ 1.58437967]\n",
      " [ 1.28326082]\n",
      " [-0.31899783]\n",
      " [-0.43633705]\n",
      " [ 0.        ]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 26 in 0.502 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 9 / timestep: 26\n",
      "Model update time: 5.317 secs\n",
      "Current Model Loss: 0.0466628\n",
      "[[ 0.04666275]\n",
      " [-0.98354292]\n",
      " [-0.36377195]\n",
      " [-0.03374073]\n",
      " [-0.26761389]\n",
      " [ 1.04726195]\n",
      " [ 1.58437967]\n",
      " [ 1.28326082]\n",
      " [-0.31899783]\n",
      " [-0.43633705]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 76 in 0.5 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 10 / timestep: 76\n",
      "Model update time: 5.871 secs\n",
      "Current Model Loss: 0.125746\n",
      "[[ 0.12574628]\n",
      " [ 0.04666275]\n",
      " [-0.98354292]\n",
      " [-0.36377195]\n",
      " [-0.03374073]\n",
      " [-0.26761389]\n",
      " [ 1.04726195]\n",
      " [ 1.58437967]\n",
      " [ 1.28326082]\n",
      " [-0.31899783]]\n",
      "\n",
      "\n",
      "Image compare checkpoint...\n",
      "Model prediction time: 0.952 secs for 10 future frames.\n",
      "Wrote new image sample checkpoint at: sample_14905732569087636.png\n",
      "\n",
      "Loaded data for timestep 11 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 11 / timestep: 11\n",
      "Model update time: 5.887 secs\n",
      "Current Model Loss: -1.44943\n",
      "[[-1.4494276 ]\n",
      " [ 0.12574628]\n",
      " [ 0.04666275]\n",
      " [-0.98354292]\n",
      " [-0.36377195]\n",
      " [-0.03374073]\n",
      " [-0.26761389]\n",
      " [ 1.04726195]\n",
      " [ 1.58437967]\n",
      " [ 1.28326082]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 92 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 12 / timestep: 92\n",
      "Model update time: 6.3 secs\n",
      "Current Model Loss: -0.807732\n",
      "[[-0.80773211]\n",
      " [-1.4494276 ]\n",
      " [ 0.12574628]\n",
      " [ 0.04666275]\n",
      " [-0.98354292]\n",
      " [-0.36377195]\n",
      " [-0.03374073]\n",
      " [-0.26761389]\n",
      " [ 1.04726195]\n",
      " [ 1.58437967]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 54 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 13 / timestep: 54\n",
      "Model update time: 5.523 secs\n",
      "Current Model Loss: 0.231201\n",
      "[[ 0.23120141]\n",
      " [-0.80773211]\n",
      " [-1.4494276 ]\n",
      " [ 0.12574628]\n",
      " [ 0.04666275]\n",
      " [-0.98354292]\n",
      " [-0.36377195]\n",
      " [-0.03374073]\n",
      " [-0.26761389]\n",
      " [ 1.04726195]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 45 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 14 / timestep: 45\n",
      "Model update time: 5.329 secs\n",
      "Current Model Loss: -0.0156541\n",
      "[[-0.01565411]\n",
      " [ 0.23120141]\n",
      " [-0.80773211]\n",
      " [-1.4494276 ]\n",
      " [ 0.12574628]\n",
      " [ 0.04666275]\n",
      " [-0.98354292]\n",
      " [-0.36377195]\n",
      " [-0.03374073]\n",
      " [-0.26761389]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 64 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 15 / timestep: 64\n",
      "Model update time: 5.879 secs\n",
      "Current Model Loss: -0.176495\n",
      "[[-0.17649454]\n",
      " [-0.01565411]\n",
      " [ 0.23120141]\n",
      " [-0.80773211]\n",
      " [-1.4494276 ]\n",
      " [ 0.12574628]\n",
      " [ 0.04666275]\n",
      " [-0.98354292]\n",
      " [-0.36377195]\n",
      " [-0.03374073]]\n",
      "\n",
      "\n",
      "Image compare checkpoint...\n",
      "Model prediction time: 1.18 secs for 10 future frames.\n",
      "Wrote new image sample checkpoint at: sample_14905732873733456.png\n",
      "\n",
      "Loaded data for timestep 105 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 16 / timestep: 105\n",
      "Model update time: 5.515 secs\n",
      "Current Model Loss: -1.00254\n",
      "[[-1.0025382 ]\n",
      " [-0.17649454]\n",
      " [-0.01565411]\n",
      " [ 0.23120141]\n",
      " [-0.80773211]\n",
      " [-1.4494276 ]\n",
      " [ 0.12574628]\n",
      " [ 0.04666275]\n",
      " [-0.98354292]\n",
      " [-0.36377195]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 144 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 17 / timestep: 144\n",
      "Model update time: 5.469 secs\n",
      "Current Model Loss: -1.07198\n",
      "[[-1.07197952]\n",
      " [-1.0025382 ]\n",
      " [-0.17649454]\n",
      " [-0.01565411]\n",
      " [ 0.23120141]\n",
      " [-0.80773211]\n",
      " [-1.4494276 ]\n",
      " [ 0.12574628]\n",
      " [ 0.04666275]\n",
      " [-0.98354292]]\n",
      "\n",
      "\n",
      "(212, 128, 128, 12)\n",
      "(212, 7)\n",
      "\n",
      "===== Training on robot sample run #:1 with num timesteps: 212\n",
      "\n",
      "\n",
      "Loaded data for timestep 163 in 0.502 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 18 / timestep: 163\n",
      "Model update time: 5.728 secs\n",
      "Current Model Loss: -1.32008\n",
      "[[-1.32008052]\n",
      " [-1.07197952]\n",
      " [-1.0025382 ]\n",
      " [-0.17649454]\n",
      " [-0.01565411]\n",
      " [ 0.23120141]\n",
      " [-0.80773211]\n",
      " [-1.4494276 ]\n",
      " [ 0.12574628]\n",
      " [ 0.04666275]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 15 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 19 / timestep: 15\n",
      "Model update time: 5.711 secs\n",
      "Current Model Loss: -1.6669\n",
      "[[-1.66689682]\n",
      " [-1.32008052]\n",
      " [-1.07197952]\n",
      " [-1.0025382 ]\n",
      " [-0.17649454]\n",
      " [-0.01565411]\n",
      " [ 0.23120141]\n",
      " [-0.80773211]\n",
      " [-1.4494276 ]\n",
      " [ 0.12574628]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 27 in 0.5 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 20 / timestep: 27\n",
      "Model update time: 5.387 secs\n",
      "Current Model Loss: -0.804886\n",
      "[[-0.80488622]\n",
      " [-1.66689682]\n",
      " [-1.32008052]\n",
      " [-1.07197952]\n",
      " [-1.0025382 ]\n",
      " [-0.17649454]\n",
      " [-0.01565411]\n",
      " [ 0.23120141]\n",
      " [-0.80773211]\n",
      " [-1.4494276 ]]\n",
      "\n",
      "\n",
      "Image compare checkpoint...\n",
      "Model prediction time: 1.333 secs for 10 future frames.\n",
      "Wrote new image sample checkpoint at: sample_14905733299339793.png\n",
      "\n",
      "Loaded data for timestep 51 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 21 / timestep: 51\n",
      "Model update time: 6.445 secs\n",
      "Current Model Loss: -0.360958\n",
      "[[-0.36095825]\n",
      " [-0.80488622]\n",
      " [-1.66689682]\n",
      " [-1.32008052]\n",
      " [-1.07197952]\n",
      " [-1.0025382 ]\n",
      " [-0.17649454]\n",
      " [-0.01565411]\n",
      " [ 0.23120141]\n",
      " [-0.80773211]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 20 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 22 / timestep: 20\n",
      "Model update time: 5.87 secs\n",
      "Current Model Loss: -1.21889\n",
      "[[-1.21888757]\n",
      " [-0.36095825]\n",
      " [-0.80488622]\n",
      " [-1.66689682]\n",
      " [-1.32008052]\n",
      " [-1.07197952]\n",
      " [-1.0025382 ]\n",
      " [-0.17649454]\n",
      " [-0.01565411]\n",
      " [ 0.23120141]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 175 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 23 / timestep: 175\n",
      "Model update time: 5.333 secs\n",
      "Current Model Loss: -1.0925\n",
      "[[-1.09250402]\n",
      " [-1.21888757]\n",
      " [-0.36095825]\n",
      " [-0.80488622]\n",
      " [-1.66689682]\n",
      " [-1.32008052]\n",
      " [-1.07197952]\n",
      " [-1.0025382 ]\n",
      " [-0.17649454]\n",
      " [-0.01565411]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 102 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 24 / timestep: 102\n",
      "Model update time: 6.102 secs\n",
      "Current Model Loss: -1.12925\n",
      "[[-1.12924528]\n",
      " [-1.09250402]\n",
      " [-1.21888757]\n",
      " [-0.36095825]\n",
      " [-0.80488622]\n",
      " [-1.66689682]\n",
      " [-1.32008052]\n",
      " [-1.07197952]\n",
      " [-1.0025382 ]\n",
      " [-0.17649454]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 32 in 0.501 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 25 / timestep: 32\n",
      "Model update time: 5.631 secs\n",
      "Current Model Loss: -0.687112\n",
      "[[-0.68711168]\n",
      " [-1.12924528]\n",
      " [-1.09250402]\n",
      " [-1.21888757]\n",
      " [-0.36095825]\n",
      " [-0.80488622]\n",
      " [-1.66689682]\n",
      " [-1.32008052]\n",
      " [-1.07197952]\n",
      " [-1.0025382 ]]\n",
      "\n",
      "\n",
      "Image compare checkpoint...\n",
      "Model prediction time: 0.918 secs for 10 future frames.\n",
      "Wrote new image sample checkpoint at: sample_1490573361497674.png\n",
      "\n",
      "Loaded data for timestep 100 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 26 / timestep: 100\n",
      "Model update time: 5.034 secs\n",
      "Current Model Loss: -1.34298\n",
      "[[-1.34297657]\n",
      " [-0.68711168]\n",
      " [-1.12924528]\n",
      " [-1.09250402]\n",
      " [-1.21888757]\n",
      " [-0.36095825]\n",
      " [-0.80488622]\n",
      " [-1.66689682]\n",
      " [-1.32008052]\n",
      " [-1.07197952]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 53 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 27 / timestep: 53\n",
      "Model update time: 5.146 secs\n",
      "Current Model Loss: -0.488012\n",
      "[[-0.48801163]\n",
      " [-1.34297657]\n",
      " [-0.68711168]\n",
      " [-1.12924528]\n",
      " [-1.09250402]\n",
      " [-1.21888757]\n",
      " [-0.36095825]\n",
      " [-0.80488622]\n",
      " [-1.66689682]\n",
      " [-1.32008052]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 69 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 28 / timestep: 69\n",
      "Model update time: 5.162 secs\n",
      "Current Model Loss: -0.435615\n",
      "[[-0.43561494]\n",
      " [-0.48801163]\n",
      " [-1.34297657]\n",
      " [-0.68711168]\n",
      " [-1.12924528]\n",
      " [-1.09250402]\n",
      " [-1.21888757]\n",
      " [-0.36095825]\n",
      " [-0.80488622]\n",
      " [-1.66689682]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 26 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 29 / timestep: 26\n",
      "Model update time: 5.074 secs\n",
      "Current Model Loss: -0.825751\n",
      "[[-0.82575053]\n",
      " [-0.43561494]\n",
      " [-0.48801163]\n",
      " [-1.34297657]\n",
      " [-0.68711168]\n",
      " [-1.12924528]\n",
      " [-1.09250402]\n",
      " [-1.21888757]\n",
      " [-0.36095825]\n",
      " [-0.80488622]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 170 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 30 / timestep: 170\n",
      "Model update time: 4.975 secs\n",
      "Current Model Loss: -1.71378\n",
      "[[-1.71377969]\n",
      " [-0.82575053]\n",
      " [-0.43561494]\n",
      " [-0.48801163]\n",
      " [-1.34297657]\n",
      " [-0.68711168]\n",
      " [-1.12924528]\n",
      " [-1.09250402]\n",
      " [-1.21888757]\n",
      " [-0.36095825]]\n",
      "\n",
      "\n",
      "Image compare checkpoint...\n",
      "Model prediction time: 0.909 secs for 10 future frames.\n",
      "Wrote new image sample checkpoint at: sample_14905733883560605.png\n",
      "\n",
      "Loaded data for timestep 163 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 31 / timestep: 163\n",
      "Model update time: 5.057 secs\n",
      "Current Model Loss: -1.55279\n",
      "[[-1.55279315]\n",
      " [-1.71377969]\n",
      " [-0.82575053]\n",
      " [-0.43561494]\n",
      " [-0.48801163]\n",
      " [-1.34297657]\n",
      " [-0.68711168]\n",
      " [-1.12924528]\n",
      " [-1.09250402]\n",
      " [-1.21888757]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 6 in 25.038 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 32 / timestep: 6\n",
      "Model update time: 5.009 secs\n",
      "Current Model Loss: -1.62886\n",
      "[[-1.62885928]\n",
      " [-1.55279315]\n",
      " [-1.71377969]\n",
      " [-0.82575053]\n",
      " [-0.43561494]\n",
      " [-0.48801163]\n",
      " [-1.34297657]\n",
      " [-0.68711168]\n",
      " [-1.12924528]\n",
      " [-1.09250402]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 56 in 3.006 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 33 / timestep: 56\n",
      "Model update time: 4.936 secs\n",
      "Current Model Loss: -0.416329\n",
      "[[-0.41632915]\n",
      " [-1.62885928]\n",
      " [-1.55279315]\n",
      " [-1.71377969]\n",
      " [-0.82575053]\n",
      " [-0.43561494]\n",
      " [-0.48801163]\n",
      " [-1.34297657]\n",
      " [-0.68711168]\n",
      " [-1.12924528]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 82 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 34 / timestep: 82\n",
      "Model update time: 4.992 secs\n",
      "Current Model Loss: -0.556532\n",
      "[[-0.55653244]\n",
      " [-0.41632915]\n",
      " [-1.62885928]\n",
      " [-1.55279315]\n",
      " [-1.71377969]\n",
      " [-0.82575053]\n",
      " [-0.43561494]\n",
      " [-0.48801163]\n",
      " [-1.34297657]\n",
      " [-0.68711168]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 101 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 35 / timestep: 101\n",
      "Model update time: 5.007 secs\n",
      "Current Model Loss: -1.2817\n",
      "[[-1.28170347]\n",
      " [-0.55653244]\n",
      " [-0.41632915]\n",
      " [-1.62885928]\n",
      " [-1.55279315]\n",
      " [-1.71377969]\n",
      " [-0.82575053]\n",
      " [-0.43561494]\n",
      " [-0.48801163]\n",
      " [-1.34297657]]\n",
      "\n",
      "\n",
      "Image compare checkpoint...\n"
     ]
    }
   ],
   "source": [
    "########################################## START THE TRAINING #######################################\n",
    "\n",
    "print(\"Running program...\")\n",
    "\n",
    "DEMONSTRATION_FOLDERS = Video_Utils.GetFoldersForRuns()\n",
    "\n",
    "NUM_DEMONSTRATIONS = len(DEMONSTRATION_FOLDERS)\n",
    "\n",
    "for f in DEMONSTRATION_FOLDERS:\n",
    "    print(f)\n",
    "\n",
    "# Separate into training and testing data based on the number of recordings (folders) we have: \n",
    "random.shuffle(DEMONSTRATION_FOLDERS)    \n",
    "lim_separate = round(NUM_DEMONSTRATIONS * PERCENT_TRAIN)    \n",
    "TRAINING_FOLDERS = DEMONSTRATION_FOLDERS[0:lim_separate]\n",
    "TESTING_FOLDERS = DEMONSTRATION_FOLDERS[lim_separate:]\n",
    "NUM_TRAINING_DEMONSTRATIONS = len(TRAINING_FOLDERS)\n",
    "NUM_TESTING_DEMONSTRATIONS = len(TESTING_FOLDERS)\n",
    "\n",
    "print(NUM_TRAINING_DEMONSTRATIONS); print(NUM_TESTING_DEMONSTRATIONS); print(NUM_DEMONSTRATIONS) # a + b = c\n",
    "\n",
    "    \n",
    "\n",
    "# Data flow process: we train on entire folder (sample run of a robot) before moving on to the next to \n",
    "# amortize the time required to load that folder's training data into RAM (multiple seconds). For a dynamics\n",
    "# model this should be perfectly acceptable because the dynamics to be learned are ideally the *same* between\n",
    "# different sample runs recorded on the (simulated) robot. \n",
    "itrs = 0\n",
    "while itrs < TOTAL_TRAINING_ITRS:\n",
    "    \n",
    "    # Choose which expert demonstration we're using: \n",
    "    CURR_DEMONSTRATION = randint(0,NUM_TRAINING_DEMONSTRATIONS-1)\n",
    "\n",
    "    frames, actions = Video_Utils.LoadFramesActionsFromFolder(TRAINING_FOLDERS[CURR_DEMONSTRATION], CAM_W, CAM_H, CAM_C*NUM_CAMS, ACTION_LEN)\n",
    "    # ^ about 5 secs\n",
    "    \n",
    "    print(frames.shape)\n",
    "    print(actions.shape)\n",
    "    \n",
    "    LENGTH_CURR_DEMONSTRATION = frames.shape[0] # number of timesteps for this demonstration\n",
    "    \n",
    "    # New demonstration, reset the relevant data buffers:\n",
    "    PREV_FRAMES_BUFFER = np.zeros((NUM_PAST_FRAMES, CAM_W, CAM_H, CAM_C*NUM_CAMS))\n",
    "    PREV_ACTION_BUFFER = np.zeros((NUM_PAST_FRAMES, ACTION_LEN))\n",
    "    FUTURE_FRAMES_BUFFER = np.zeros((NUM_FUTURE_FRAMES, CAM_W, CAM_H, CAM_C*NUM_CAMS))\n",
    "    FUTURE_ACTION_BUFFER = np.zeros((NUM_FUTURE_FRAMES-1, ACTION_LEN))\n",
    "    \n",
    "    print(\"\\n===== Training on robot sample run #:\"+str(CURR_DEMONSTRATION)+\" with num timesteps: \"+str(LENGTH_CURR_DEMONSTRATION)+\"\\n\")\n",
    "    \n",
    "    for i in range(0, round(LENGTH_CURR_DEMONSTRATION/NUM_FUTURE_FRAMES)): \n",
    "        # only sample a limited number from each demo before moving on to next demo. \n",
    "        t = randint(0,LENGTH_CURR_DEMONSTRATION-1)\t# now we have random shuffle training within a demo.\n",
    "            \n",
    "        ##### STEP 1: Load Buffers with Data #####    \n",
    "        ms1 = time.time()*1000.0 \n",
    "        # Past data: (up to and including x(t), and u(t) - which causes x(t+1), etc)\n",
    "        if (t >= NUM_PAST_FRAMES): # regular, in-bounds\n",
    "            PREV_FRAMES_BUFFER = frames[t-NUM_PAST_FRAMES:t, :] \n",
    "            PREV_ACTION_BUFFER_BUFFER = actions[t-NUM_PAST_FRAMES:t, :]\n",
    "        else: # t is less than past frames, need to concat zeros at beginning\n",
    "            LIM1 = np.abs(t - NUM_PAST_FRAMES) \n",
    "            PREV_FRAMES_BUFFER = np.concatenate((np.zeros((LIM1, CAM_W, CAM_H, CAM_C*NUM_CAMS)), frames[0:t, :]), axis=0) \n",
    "            PREV_ACTIONS_BUFFER = np.concatenate((np.zeros((LIM1, ACTION_LEN)), actions[0:t, :]), axis=0)\n",
    "        # Future Data: (include 1 less future action than frame to retain logical p(x(t+1)|x(t),u(t)) structure)\n",
    "        if (t <= LENGTH_CURR_DEMONSTRATION - NUM_FUTURE_FRAMES - 1):\n",
    "            FUTURE_FRAMES_BUFFER = frames[t:t+NUM_FUTURE_FRAMES, :]\n",
    "            FUTURE_ACTIONS_BUFFER = actions[t:t+NUM_FUTURE_FRAMES-1, :]\n",
    "        else: # running off the end\n",
    "            LIM2 = t + NUM_FUTURE_FRAMES - LENGTH_CURR_DEMONSTRATION \n",
    "            # ^ We need to repeat last frame this many times / add this many zero actions\n",
    "            FUTURE_FRAMES_BUFFER = frames[t:LENGTH_CURR_DEMONSTRATION, :]\n",
    "            last_frame = np.expand_dims(frames[LENGTH_CURR_DEMONSTRATION-1], axis=0)\n",
    "            for j in range(0,LIM2):\n",
    "                FUTURE_FRAMES_BUFFER = np.concatenate((FUTURE_FRAMES_BUFFER, last_frame), axis=0) \n",
    "            FUTURE_ACTIONS_BUFFER = np.concatenate((actions[t:LENGTH_CURR_DEMONSTRATION, :], np.zeros((LIM2, ACTION_LEN))), axis=0)\n",
    "        ms2 = time.time()*1000.0\n",
    "        print(\"\\nLoaded data for timestep \"+str(t)+\" in \"+str(round((ms2-ms1)/1, 3))+\" msecs\\n\")\n",
    "        \n",
    "        #print(PREV_FRAMES_BUFFER.shape)\n",
    "        #print(PREV_ACTION_BUFFER.shape)\n",
    "        #print(FUTURE_FRAMES_BUFFER.shape)\n",
    "        #print(FUTURE_ACTION_BUFFER.shape)\n",
    "        \n",
    "        ##### STEP 2: Train on Batch of Data Gathered Above #####\n",
    "        ms1 = time.time()*1000.0\n",
    "        # inputs: [input_prev_frames_raw, input_prev_actions_raw, input_future_actions_raw]\n",
    "        # outputs/training targets: [future_frames]\n",
    "        model_loss = UDM_model.train_on_batch(\n",
    "                     [np.expand_dims(PREV_FRAMES_BUFFER.reshape(CAM_W, CAM_H, CAM_C*NUM_CAMS*NUM_PAST_FRAMES), axis=0), \n",
    "                      np.expand_dims(PREV_ACTION_BUFFER.reshape(-1), axis=0), \n",
    "                      np.expand_dims(FUTURE_ACTION_BUFFER.reshape(-1), axis=0)], # <--- inputs \n",
    "                     [np.expand_dims(FUTURE_FRAMES_BUFFER.reshape(CAM_W, CAM_H, CAM_C*NUM_CAMS*NUM_PAST_FRAMES), axis=0)])\n",
    "        \n",
    "        ms2 = time.time()*1000.0\n",
    "        print(\"===================================================\")\n",
    "        print(\"iter: \" + str(itrs) + \" / timestep: \"+str(t))\n",
    "        print(\"Model update time: \"+str(round((ms2-ms1)/1000, 3)) + ' secs')\n",
    "        print(\"Current Model Loss: \"+str(model_loss))\n",
    "        MODEL_LOSS_BUFFER = np.roll(MODEL_LOSS_BUFFER, 1); MODEL_LOSS_BUFFER[0] = model_loss\n",
    "        print(MODEL_LOSS_BUFFER); print(\"\\n\")\n",
    "        \n",
    "        ##### STEP 3: CHECKPOINTS #####\n",
    "        \n",
    "        if (itrs % IMAGE_COMPARE_CHECKPOINT == 0 ): # View how the model is doing\n",
    "            print(\"Image compare checkpoint...\")\n",
    "            timestamp = str(time.time()).replace(\".\",\"\")\n",
    "            \n",
    "            # Get the generated future frames: \n",
    "            ms1 = time.time()*1000.0\n",
    "            GEN_FUTURE_FRAMES = UDM_model.predict(\n",
    "                                [np.expand_dims(PREV_FRAMES_BUFFER.reshape(CAM_W, CAM_H, CAM_C*NUM_CAMS*NUM_PAST_FRAMES), axis=0), \n",
    "                                 np.expand_dims(PREV_ACTION_BUFFER.reshape(-1), axis=0), \n",
    "                                 np.expand_dims(FUTURE_ACTION_BUFFER.reshape(-1), axis=0)])\n",
    "            GEN_FUTURE_FRAMES = GEN_FUTURE_FRAMES[0]\n",
    "            ms2 = time.time()*1000.0\n",
    "            print(\"Model prediction time: \"+str(round((ms2-ms1)/1000, 3)) +' secs'+' for '+str(NUM_FUTURE_FRAMES)+' future frames.')\n",
    "            \n",
    "            imgs_gen = Video_Utils.ViewFutureFrames(GEN_FUTURE_FRAMES.reshape(NUM_FUTURE_FRAMES, CAM_W, CAM_H, CAM_C*NUM_CAMS))\n",
    "            imgs_split = np.zeros((30, NUM_FUTURE_FRAMES*CAM_W, 3)) # goes in between to separate real/generated images\n",
    "            imgs_gt = Video_Utils.ViewFutureFrames(FUTURE_FRAMES_BUFFER)\n",
    "            #print(imgs_gt.shape); print(imgs_split.shape); print(imgs_gen.shape);\n",
    "            imgs_compare = np.concatenate((imgs_gt, imgs_split, imgs_gen),axis=0)\n",
    "            #print(imgs_compare.shape)\n",
    "            #cv2.imshow('image',imgs_compare) \n",
    "            #cv2.waitKey(0)\n",
    "            img_filename = 'sample_'+timestamp+'.png' \n",
    "            cv2.imwrite(img_filename,(imgs_compare*255 + 127.5))\n",
    "            print(\"Wrote new image sample checkpoint at: \"+img_filename)\n",
    "            \n",
    "        if (itrs % SAVE_CHECKPOINT_ITRS == 0): # save progress\n",
    "            print(\"Model save checkpoint, itr: \"+str(itrs))\n",
    "            ms1 = time.time()*1000.0\n",
    "            timestamp = str(time.time()).replace(\".\",\"\")\n",
    "            mean_recent_loss = round(np.mean(MODEL_LOSS_BUFFER), 4)\n",
    "            model_str_name = 'UDM_weights_'+str(mean_recent_loss)+'.h5' # pro-tip: manually re-name after each run... \n",
    "            UDM_model.save(model_str_name)\n",
    "            ms2 = time.time()*1000.0\n",
    "            print(\"Model save time: \"+str(round((ms2-ms1)/1000, 3)) +' secs')\n",
    "\n",
    "        itrs = itrs + 1 # don't forget to increment total training itrs counter    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1054, 1280, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (1054,1280,3) into shape (1280,1054,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e21b5b89ab66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m################## make a video... #############\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvid_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVideo_Utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGetDataFromFolder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'old_visuals'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdo_normalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1280\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1054\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvid_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\JDS\\Desktop\\AI_Software\\Video_Utils.py\u001b[0m in \u001b[0;36mGetDataFromFolder\u001b[0;34m(folder, do_normalize, w, h)\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#cv2.resize(cv2.imread(f), (w, h))\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                 \u001b[0mimgs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m                 \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (1054,1280,3) into shape (1280,1054,3)"
     ]
    }
   ],
   "source": [
    "################## make a video... #############\n",
    "\n",
    "vid_data = Video_Utils.GetDataFromFolder('old_visuals', do_normalize=0, w=1280, h=1054)\n",
    "print(vid_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
