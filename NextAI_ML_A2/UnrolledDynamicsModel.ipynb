{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFrom the command line you can convert a notebook to python with this command:\\n\\nipython nbconvert --to python <YourNotebook>.ipynb\\n\\nYou may have to install the python mistune package:\\n\\nsudo pip install mistune\\n'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os \n",
    "import random\n",
    "from random import randint\n",
    "import numpy as np\n",
    "import cv2\n",
    "sys.path.append('..')\n",
    "import Video_Utils\n",
    "import CNN_Utils\n",
    "import h5py\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Convolution2D, UpSampling2D, MaxPooling2D, ZeroPadding2D, Reshape, merge\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "from keras.layers.core import Lambda, Merge\n",
    "from keras.engine import Layer\n",
    "\n",
    "'''\n",
    "From the command line you can convert a notebook to python with this command:\n",
    "\n",
    "ipython nbconvert --to python <YourNotebook>.ipynb\n",
    "\n",
    "You may have to install the python mistune package:\n",
    "\n",
    "sudo pip install mistune\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################## GLOBAL VARIABLES #########################################\n",
    "# These variables are specific to the dataset of the problem. \n",
    "# Our data is the following: \n",
    "#\n",
    "# time-series of 4 cameras (128 x 128 x 3) looking at a robot complete a task (pick up object, place in bin)\n",
    "# + corresponding time series of 7-d vector of joint commands (6 DOF + gripper) denoting the position delta command.\n",
    "#\n",
    "# In future comments, we will refer to the states and actions as x(t) and u(t) respectively, where 'state' \n",
    "# denotes the visual data sensed from the world (the 4 cameras) and action is the 7-d joint position delta vector. \n",
    "#\n",
    "# Data was generated and collected using the V-REP (http://www.coppeliarobotics.com/) robot simulator \n",
    "# (free/education version). Please contact the author of this notebook for the specific scene and data-generation\n",
    "# script \n",
    "\n",
    "global CAM_W\n",
    "global CAM_H\n",
    "global CAM_C\n",
    "global NUM_CAMS\n",
    "global ACTION_LEN\n",
    "\n",
    "CAM_W = 128\n",
    "CAM_H = 128\n",
    "CAM_C = 3\n",
    "NUM_CAMS = 4\n",
    "ACTION_LEN = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################################### MODEL PARAMETERS #########################################\n",
    "global NUM_FUTURE_FRAMES\n",
    "global NUM_PAST_FRAMES\n",
    "#global NUM_VGG_FEAT_MAPS\n",
    "#global VGG_FEAT_W\n",
    "#global VGG_FEAT_H \n",
    "global CONV1_FEAT_MAPS\n",
    "global CONV2_FEAT_MAPS\n",
    "global CONV3_FEAT_MAPS\n",
    "global CONVP1_FEAT_MAPS \n",
    "global CONVP2_FEAT_MAPS\n",
    "global CONVP3_FEAT_MAPS\n",
    "global CONVF1_FEAT_MAPS\n",
    "global CONVF2_FEAT_MAPS\n",
    "global CONVF3_FEAT_MAPS\n",
    "global CONVD1_FEAT_MAPS\n",
    "global CONVD2_FEAT_MAPS\n",
    "global CONVD3_FEAT_MAPS\n",
    "global D_DENSE1\n",
    "global D_DENSE2\n",
    "global D_DENSE3\n",
    "global UDM_model       # The actual Keras model\n",
    "\n",
    "#VGG_FEAT_W = 64        # using \"block1_pool\"\n",
    "#VGG_FEAT_H = 64        # using \"block1_pool\"  \n",
    "#NUM_VGG_FEAT_MAPS = 64 # using \"block1_pool\" (per camera)\n",
    "NUM_FUTURE_FRAMES = 10\n",
    "NUM_PAST_FRAMES = 10\n",
    "# Below: these layers create a more data-efficient representation of past from preprocessed features\n",
    "CONVP1_FEAT_MAPS = round(CAM_C*NUM_CAMS*NUM_PAST_FRAMES/2)        # e.g. 120/2 = 60\n",
    "CONVP2_FEAT_MAPS = round(CONVP1_FEAT_MAPS/2)                      # e.g. 30\n",
    "CONVP3_FEAT_MAPS = round(CONVP2_FEAT_MAPS/2)                      # e.g. 15\n",
    "# Below: these layers create a more data-efficient representation from merged frame,action [from the past] inputs\n",
    "CONV1_FEAT_MAPS = CONVP3_FEAT_MAPS\n",
    "CONV2_FEAT_MAPS = CONVP3_FEAT_MAPS\n",
    "CONV3_FEAT_MAPS = NUM_PAST_FRAMES\n",
    "# Below: these are final output layers that transform merged input data to predicted output frames\n",
    "CONVF1_FEAT_MAPS = (CONV3_FEAT_MAPS + 1)*2                        # e.g. 22\n",
    "CONVF2_FEAT_MAPS = round(CONVF1_FEAT_MAPS*2)                      # e.g. 48\n",
    "CONVF3_FEAT_MAPS = round(CONVF2_FEAT_MAPS*2)                      # e.g. 96\n",
    "CONVF4_FEAT_MAPS = round(CAM_C*NUM_CAMS*NUM_PAST_FRAMES)  # e.g. 3*4*NUM_PAST_FRAMES = 120 \n",
    "# Below: parameters to learn important features in the sequence of future actions \n",
    "D_DENSE3 = CAM_W * CAM_H\n",
    "D_DENSE2 = round(D_DENSE3 / 8)\n",
    "D_DENSE1 = round(D_DENSE2 / 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_prev_frames_raw (InputLaye (None, 128, 128, 120) 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_53 (Convolution2D) (None, 128, 128, 60)  64860       input_prev_frames_raw[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "input_prev_actions_raw (InputLay (None, 70)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_54 (Convolution2D) (None, 128, 128, 30)  16230       convolution2d_53[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dense_17 (Dense)                 (None, 16384)         1163264     input_prev_actions_raw[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_55 (Convolution2D) (None, 128, 128, 15)  4065        convolution2d_54[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "input_prev_actions_p_r (Reshape) (None, 128, 128, 1)   0           dense_17[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "input_future_actions_raw (InputL (None, 63)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "merged_prev_inputs (Merge)       (None, 128, 128, 16)  0           convolution2d_55[0][0]           \n",
      "                                                                   input_prev_actions_p_r[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "D_A1 (Dense)                     (None, 256)           16384       input_future_actions_raw[0][0]   \n",
      "____________________________________________________________________________________________________\n",
      "merged_lvl_1 (Convolution2D)     (None, 128, 128, 15)  11775       merged_prev_inputs[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "D_A2 (Dense)                     (None, 2048)          526336      D_A1[0][0]                       \n",
      "____________________________________________________________________________________________________\n",
      "merged_lvl_2 (Convolution2D)     (None, 128, 128, 15)  2040        merged_lvl_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "D_A3 (Dense)                     (None, 16384)         33570816    D_A2[0][0]                       \n",
      "____________________________________________________________________________________________________\n",
      "merged_lvl_3 (Convolution2D)     (None, 128, 128, 10)  1360        merged_lvl_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "action_branch_r (Reshape)        (None, 128, 128, 1)   0           D_A3[0][0]                       \n",
      "____________________________________________________________________________________________________\n",
      "merged_efficient_inputs_all (Mer (None, 128, 128, 11)  0           merged_lvl_3[0][0]               \n",
      "                                                                   action_branch_r[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "Out_C1 (Convolution2D)           (None, 128, 128, 22)  11880       merged_efficient_inputs_all[0][0]\n",
      "____________________________________________________________________________________________________\n",
      "Out_C2 (Convolution2D)           (None, 128, 128, 44)  8756        Out_C1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "Out_C3 (Convolution2D)           (None, 128, 128, 88)  34936       Out_C2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "Out_C4 (Convolution2D)           (None, 128, 128, 120) 95160       Out_C3[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 35,527,862\n",
      "Trainable params: 35,527,862\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "################################################# THE MODEL ####################################################\n",
    "'''\n",
    "An explanation of the dynamics model and the buffers: \n",
    "\n",
    "We are training a model to produce p(x(t+1: t+F) | x(t-P: t), u(t-P: t), u(t+1: t+F-1)) \n",
    "\n",
    "I.e.: predict the future F frames, \n",
    "      given the past P frames, past P actions, and the future F-1 actions \n",
    "      \n",
    "This can be viewed as unrolling the classic p(x(t+1) | x(t), u(t)) dynamics model. \n",
    "\n",
    "We have proposed this 'unrolled dynamics' problem formulation for the following reasons:\n",
    "\n",
    "1) Conditioning on longer sequences to capture the effects of repeated actions, reduce discretization errors\n",
    "2) Improved training due to longer sequences. Single-step visual models have a tendency to \"copy\" prev. frame\n",
    "   as the output\n",
    "3) Be able to visualize a future state trajectory based on a proposed set of actions.\n",
    "   ---> this has applications in control planning, and can be useful to a human operator/co-worker, or for\n",
    "        another system to check for safety hazards, etc. Plus makes a sick demo (\"this is what the robot is\n",
    "        thinking\") lol. \n",
    "        \n",
    "PREV_FRAMES_BUFFER ---> input, this is x(t-P: t)         (size P,W,H,3*num_cams)\n",
    "PREV_ACTION_BUFFER ---> input, this is u(t-P: t)         (size P,A)\n",
    "FUTURE_FRAMES_BUFFER ---> output, this is x(t+1: t+F)    (size F,W,H,3*num_cams)\n",
    "FUTURE_ACTION_BUFFER ---> input, this is u(t+1: t+F-1)   (size F-1,A)           [if this was output, we'd be doing control]\n",
    "\n",
    "(note: visual inputs may be pre-processed with another pre-trained model)\n",
    "\n",
    "Other details/throughts: \n",
    "- ELU activations: to capture training efficiency of ReLU while reducing need for batch normalization\n",
    "- multi-frame unrolling vs. single frame p(x(t+1)|x(t),u(t)): unrolling is like doing batches of the single-frame\n",
    "  version, except the model can correlated (x,y)_t points in the batch   \n",
    "- The size of our conv layers (measured by # of filters per layer) creates a \"bottleneck\": form a more data-efficient\n",
    "  representation of the past information, combine with future action information, and then generate the larger tensor\n",
    "  of future predicted frames. \n",
    "\n",
    "'''\n",
    "# Can use part of pre-trained VGG model to seed features with reasonable features: (used online during training)  \n",
    "#vgg_preprocessor = CNN_Utils.GetVGGModel(\"block1_pool\", CAM_W, CAM_H, print_timing=1) \n",
    "# ^ Note: not using for this experiment (creates too many feature maps per camera for current hardware)\n",
    "\n",
    "# Model input #1: past frames\n",
    "input_prev_frames_raw = Input(shape=(CAM_W, CAM_H, CAM_C*NUM_CAMS*NUM_PAST_FRAMES), name='input_prev_frames_raw')\n",
    "# below: some layers to learn what information is important from the past\n",
    "input_prev_frames = Convolution2D(CONVP1_FEAT_MAPS, 3, 3, activation='elu', border_mode='same')(input_prev_frames_raw)\n",
    "input_prev_frames = Convolution2D(CONVP2_FEAT_MAPS, 3, 3, activation='elu', border_mode='same')(input_prev_frames)\n",
    "input_prev_frames = Convolution2D(CONVP3_FEAT_MAPS, 3, 3, activation='elu', border_mode='same')(input_prev_frames) \n",
    "\n",
    "# Model input #1: past actions \n",
    "input_prev_actions_raw = Input(shape=(NUM_PAST_FRAMES*ACTION_LEN,), name='input_prev_actions_raw')\n",
    "# Below: project and reshape, so we can merge inputs later: \n",
    "input_prev_actions_p = Dense(output_dim=CAM_W*CAM_H*1, activation='elu')(input_prev_actions_raw)\n",
    "input_prev_actions_p_r = Reshape((CAM_W, CAM_H, 1), name='input_prev_actions_p_r')(input_prev_actions_p)\n",
    "\n",
    "# Model input #3: future actions\n",
    "input_future_actions_raw = Input(shape=((NUM_FUTURE_FRAMES-1)*ACTION_LEN,), name='input_future_actions_raw')\n",
    "# format for convenience, to let us 'pick off' actions and sequentially predict the next frame logically\n",
    "# Below: these parameters gather information for an action pertaining to how it affects a future state\n",
    "D_A1 = Dense(output_dim=D_DENSE1, activation='elu', name='D_A1')(input_future_actions_raw)\n",
    "D_A2 = Dense(output_dim=D_DENSE2, activation='elu', name='D_A2')(D_A1)\n",
    "D_A3 = Dense(output_dim=D_DENSE3, activation='elu', name='D_A3')(D_A2)\n",
    "future_action_branch = Reshape((CAM_W, CAM_H, 1), name='action_branch_r')(D_A3)\n",
    "\n",
    "# MERGE PAST INPUTS: \n",
    "merged_input = merge([input_prev_frames, input_prev_actions_p_r], \n",
    "                      mode='concat', concat_axis=3, name='merged_prev_inputs')\n",
    "# ^ so now past frames, info from past actions has been merged into a tensor that is size e.g.:\n",
    "# [W, H, CAM_C*NUM_CAMS*NUM_PAST_FRAMES/8 + 1] (e.g. == 16 for 10 past frames)\n",
    "\n",
    "# Pre-process merged inputs that contain past information:\n",
    "# This is meant to form a more efficient representation to be used in predicting future frames given proposed actions:\n",
    "merged_input = Convolution2D(CONV1_FEAT_MAPS, 7, 7, activation='elu', border_mode='same', name='merged_lvl_1')(merged_input)\n",
    "merged_input = Convolution2D(CONV2_FEAT_MAPS, 3, 3, activation='elu', border_mode='same', name='merged_lvl_2')(merged_input)\n",
    "merged_input = Convolution2D(CONV3_FEAT_MAPS, 3, 3, activation='elu', border_mode='same', name='merged_lvl_3')(merged_input)\n",
    "\n",
    "# Now merge with information about future actions: \n",
    "merged_input = merge([merged_input, future_action_branch], \n",
    "                      mode='concat', concat_axis=3, name='merged_efficient_inputs_all')\n",
    "# ^ This will have size [W, H, NUM_PAST_FRAMES + 1], e.g. [128, 128, 11]\n",
    "\n",
    "# Conv layers for predicting the next frames given all relevant data: \n",
    "Out_C1 = Convolution2D(CONVF1_FEAT_MAPS, 7, 7, activation='elu', border_mode='same', name='Out_C1')(merged_input)\n",
    "Out_C2 = Convolution2D(CONVF2_FEAT_MAPS, 3, 3, activation='elu', border_mode='same', name='Out_C2')(Out_C1)\n",
    "Out_C3 = Convolution2D(CONVF3_FEAT_MAPS, 3, 3, activation='elu', border_mode='same', name='Out_C3')(Out_C2)\n",
    "Out_C4 = Convolution2D(CONVF4_FEAT_MAPS, 3, 3, activation='elu', border_mode='same', name='Out_C4')(Out_C3)\n",
    "# ^ Out_C4 is a final output layer, spits out a frame (defined as WxHx3*num_cams*num_future_frames)\n",
    "\n",
    "# Define the full model structure: \n",
    "model_inputs = [input_prev_frames_raw, input_prev_actions_raw, input_future_actions_raw]\n",
    "# Review of input dimensions: (caller of train/predict must expand_dims to achieve shapes)\n",
    "# input_prev_frames_raw: [W, H, 3*NUM_CAMS*NUM_PAST_FRAMES] e.g. [1, 128, 128, 120]\n",
    "# input_prev_actions_raw: [NUM_PAST_FRAMES*ACTION_LEN,] e.g. [1, 70] \n",
    "# input_future_actions_raw: [(NUM_FUTURE_FRAMES-1)*ACTION_LEN,] e.g. [1, 63]\n",
    "model_outputs = [Out_C4] \n",
    "# Target output data: \n",
    "# Out_C4: [W, H, 3*NUM_CAMS*NUM_FUTURE_FRAMES], e.g. [1, 128, 128, 120]\n",
    "UDM_model = Model(input=model_inputs, output=model_outputs)\n",
    "\n",
    "UDM_optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "UDM_model.compile(loss='binary_crossentropy', optimizer=UDM_optimizer) \n",
    "# ^ TODO: custom loss function more appropriate for sequence of similar images...   \n",
    "\n",
    "UDM_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################################### INITIALIZATION, SETUP OF MODEL #########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########################################## Globals for training params ####################################\n",
    "\n",
    "global TOTAL_TRAINING_ITRS\n",
    "global SAVE_CHECKPOINT_ITRS\n",
    "global NUM_DEMONSTRATIONS\n",
    "global CURR_DEMONSTRATION\n",
    "global LENGTH_CURR_DEMONSTRATION # e.g. current demo we're looking at is 230 timesteps\n",
    "global T_CURR_DEMONSTRATION      # and e.g. we're currently on timestep 87\n",
    "global PERCENT_TRAIN             # percent of data used for training vs. valudation\n",
    "global DEMONSTRATION_FOLDERS\n",
    "global TRAINING_FOLDERS\n",
    "global TESTING_FOLDERS\n",
    "global NUM_TRAINING_DEMONSTRATIONS\n",
    "global NUM_TESTING_DEMONSTRATIONS\n",
    "global IMAGE_COMPARE_CHECKPOINT\n",
    "\n",
    "global PREV_FRAMES_BUFFER\n",
    "global PREV_ACTION_BUFFER\n",
    "global FUTURE_FRAMES_BUFFER\n",
    "global FUTURE_ACTION_BUFFER\n",
    "global MODEL_LOSS_BUFFER\n",
    "\n",
    "PREV_FRAMES_BUFFER = np.zeros((NUM_PAST_FRAMES, CAM_W, CAM_H, CAM_C))\n",
    "PREV_ACTION_BUFFER = np.zeros((NUM_PAST_FRAMES, ACTION_LEN))\n",
    "FUTURE_FRAMES_BUFFER = np.zeros((NUM_FUTURE_FRAMES, CAM_W, CAM_H, CAM_C))\n",
    "FUTURE_ACTION_BUFFER = np.zeros((NUM_FUTURE_FRAMES-1, ACTION_LEN))\n",
    "\n",
    "TOTAL_TRAINING_ITRS = 100000\n",
    "SAVE_CHECKPOINT_ITRS = 100\n",
    "IMAGE_COMPARE_CHECKPOINT = 5\n",
    "CURR_DEMONSTRATION = -1 # start on the first one, loop to another one each itr\n",
    "LENGTH_CURR_DEMONSTRATION = -1\n",
    "PERCENT_TRAIN = 0.75\n",
    "MODEL_LOSS_BUFFER = np.zeros((10,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running program...\n",
      "u_sequence_1486408000621\n",
      "u_sequence_1486420877768\n",
      "u_sequence_1486421372155\n",
      "u_sequence_1486422003939\n",
      "u_sequence_1486423780289\n",
      "u_sequence_1486424226636\n",
      "u_sequence_1486424664609\n",
      "u_sequence_1486425113072\n",
      "u_sequence_1486425723665\n",
      "u_sequence_1486426394928\n",
      "u_sequence_1486483126414\n",
      "u_sequence_1486483372904\n",
      "u_sequence_1486483607589\n",
      "u_sequence_1486483842933\n",
      "u_sequence_1486484146973\n",
      "u_sequence_1486484663068\n",
      "u_sequence_1486485509467\n",
      "u_sequence_1486485832231\n",
      "u_sequence_1486486076841\n",
      "u_sequence_1486486803219\n",
      "15\n",
      "5\n",
      "20\n",
      "(185, 128, 128, 12)\n",
      "(185, 7)\n",
      "\n",
      "===== Training on robot sample run #:13 with num timesteps: 185\n",
      "\n",
      "\n",
      "Loaded data for timestep 8 in 24.032 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 0 / timestep: 8\n",
      "Model update time: 15.453 secs\n",
      "Current Model Loss: -0.436337\n",
      "[[-0.43633705]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n",
      "\n",
      "\n",
      "Image compare checkpoint...\n",
      "Model prediction time: 1.566 secs for 10 future frames.\n",
      "Wrote new image sample checkpoint at: sample_14905731885338917.png\n",
      "Model save checkpoint, itr: 0\n",
      "Model save time: 7.873 secs\n",
      "\n",
      "Loaded data for timestep 95 in 4.007 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 1 / timestep: 95\n",
      "Model update time: 5.365 secs\n",
      "Current Model Loss: -0.318998\n",
      "[[-0.31899783]\n",
      " [-0.43633705]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 80 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 2 / timestep: 80\n",
      "Model update time: 5.894 secs\n",
      "Current Model Loss: 1.28326\n",
      "[[ 1.28326082]\n",
      " [-0.31899783]\n",
      " [-0.43633705]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 73 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 3 / timestep: 73\n",
      "Model update time: 5.186 secs\n",
      "Current Model Loss: 1.58438\n",
      "[[ 1.58437967]\n",
      " [ 1.28326082]\n",
      " [-0.31899783]\n",
      " [-0.43633705]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 74 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 4 / timestep: 74\n",
      "Model update time: 5.263 secs\n",
      "Current Model Loss: 1.04726\n",
      "[[ 1.04726195]\n",
      " [ 1.58437967]\n",
      " [ 1.28326082]\n",
      " [-0.31899783]\n",
      " [-0.43633705]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 111 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 5 / timestep: 111\n",
      "Model update time: 5.748 secs\n",
      "Current Model Loss: -0.267614\n",
      "[[-0.26761389]\n",
      " [ 1.04726195]\n",
      " [ 1.58437967]\n",
      " [ 1.28326082]\n",
      " [-0.31899783]\n",
      " [-0.43633705]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n",
      "\n",
      "\n",
      "Image compare checkpoint...\n",
      "Model prediction time: 1.088 secs for 10 future frames.\n",
      "Wrote new image sample checkpoint at: sample_14905732261980016.png\n",
      "\n",
      "Loaded data for timestep 32 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 6 / timestep: 32\n",
      "Model update time: 6.212 secs\n",
      "Current Model Loss: -0.0337407\n",
      "[[-0.03374073]\n",
      " [-0.26761389]\n",
      " [ 1.04726195]\n",
      " [ 1.58437967]\n",
      " [ 1.28326082]\n",
      " [-0.31899783]\n",
      " [-0.43633705]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 129 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 7 / timestep: 129\n",
      "Model update time: 5.699 secs\n",
      "Current Model Loss: -0.363772\n",
      "[[-0.36377195]\n",
      " [-0.03374073]\n",
      " [-0.26761389]\n",
      " [ 1.04726195]\n",
      " [ 1.58437967]\n",
      " [ 1.28326082]\n",
      " [-0.31899783]\n",
      " [-0.43633705]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 93 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 8 / timestep: 93\n",
      "Model update time: 5.671 secs\n",
      "Current Model Loss: -0.983543\n",
      "[[-0.98354292]\n",
      " [-0.36377195]\n",
      " [-0.03374073]\n",
      " [-0.26761389]\n",
      " [ 1.04726195]\n",
      " [ 1.58437967]\n",
      " [ 1.28326082]\n",
      " [-0.31899783]\n",
      " [-0.43633705]\n",
      " [ 0.        ]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 26 in 0.502 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 9 / timestep: 26\n",
      "Model update time: 5.317 secs\n",
      "Current Model Loss: 0.0466628\n",
      "[[ 0.04666275]\n",
      " [-0.98354292]\n",
      " [-0.36377195]\n",
      " [-0.03374073]\n",
      " [-0.26761389]\n",
      " [ 1.04726195]\n",
      " [ 1.58437967]\n",
      " [ 1.28326082]\n",
      " [-0.31899783]\n",
      " [-0.43633705]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 76 in 0.5 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 10 / timestep: 76\n",
      "Model update time: 5.871 secs\n",
      "Current Model Loss: 0.125746\n",
      "[[ 0.12574628]\n",
      " [ 0.04666275]\n",
      " [-0.98354292]\n",
      " [-0.36377195]\n",
      " [-0.03374073]\n",
      " [-0.26761389]\n",
      " [ 1.04726195]\n",
      " [ 1.58437967]\n",
      " [ 1.28326082]\n",
      " [-0.31899783]]\n",
      "\n",
      "\n",
      "Image compare checkpoint...\n",
      "Model prediction time: 0.952 secs for 10 future frames.\n",
      "Wrote new image sample checkpoint at: sample_14905732569087636.png\n",
      "\n",
      "Loaded data for timestep 11 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 11 / timestep: 11\n",
      "Model update time: 5.887 secs\n",
      "Current Model Loss: -1.44943\n",
      "[[-1.4494276 ]\n",
      " [ 0.12574628]\n",
      " [ 0.04666275]\n",
      " [-0.98354292]\n",
      " [-0.36377195]\n",
      " [-0.03374073]\n",
      " [-0.26761389]\n",
      " [ 1.04726195]\n",
      " [ 1.58437967]\n",
      " [ 1.28326082]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 92 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 12 / timestep: 92\n",
      "Model update time: 6.3 secs\n",
      "Current Model Loss: -0.807732\n",
      "[[-0.80773211]\n",
      " [-1.4494276 ]\n",
      " [ 0.12574628]\n",
      " [ 0.04666275]\n",
      " [-0.98354292]\n",
      " [-0.36377195]\n",
      " [-0.03374073]\n",
      " [-0.26761389]\n",
      " [ 1.04726195]\n",
      " [ 1.58437967]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 54 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 13 / timestep: 54\n",
      "Model update time: 5.523 secs\n",
      "Current Model Loss: 0.231201\n",
      "[[ 0.23120141]\n",
      " [-0.80773211]\n",
      " [-1.4494276 ]\n",
      " [ 0.12574628]\n",
      " [ 0.04666275]\n",
      " [-0.98354292]\n",
      " [-0.36377195]\n",
      " [-0.03374073]\n",
      " [-0.26761389]\n",
      " [ 1.04726195]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 45 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 14 / timestep: 45\n",
      "Model update time: 5.329 secs\n",
      "Current Model Loss: -0.0156541\n",
      "[[-0.01565411]\n",
      " [ 0.23120141]\n",
      " [-0.80773211]\n",
      " [-1.4494276 ]\n",
      " [ 0.12574628]\n",
      " [ 0.04666275]\n",
      " [-0.98354292]\n",
      " [-0.36377195]\n",
      " [-0.03374073]\n",
      " [-0.26761389]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 64 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 15 / timestep: 64\n",
      "Model update time: 5.879 secs\n",
      "Current Model Loss: -0.176495\n",
      "[[-0.17649454]\n",
      " [-0.01565411]\n",
      " [ 0.23120141]\n",
      " [-0.80773211]\n",
      " [-1.4494276 ]\n",
      " [ 0.12574628]\n",
      " [ 0.04666275]\n",
      " [-0.98354292]\n",
      " [-0.36377195]\n",
      " [-0.03374073]]\n",
      "\n",
      "\n",
      "Image compare checkpoint...\n",
      "Model prediction time: 1.18 secs for 10 future frames.\n",
      "Wrote new image sample checkpoint at: sample_14905732873733456.png\n",
      "\n",
      "Loaded data for timestep 105 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 16 / timestep: 105\n",
      "Model update time: 5.515 secs\n",
      "Current Model Loss: -1.00254\n",
      "[[-1.0025382 ]\n",
      " [-0.17649454]\n",
      " [-0.01565411]\n",
      " [ 0.23120141]\n",
      " [-0.80773211]\n",
      " [-1.4494276 ]\n",
      " [ 0.12574628]\n",
      " [ 0.04666275]\n",
      " [-0.98354292]\n",
      " [-0.36377195]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 144 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 17 / timestep: 144\n",
      "Model update time: 5.469 secs\n",
      "Current Model Loss: -1.07198\n",
      "[[-1.07197952]\n",
      " [-1.0025382 ]\n",
      " [-0.17649454]\n",
      " [-0.01565411]\n",
      " [ 0.23120141]\n",
      " [-0.80773211]\n",
      " [-1.4494276 ]\n",
      " [ 0.12574628]\n",
      " [ 0.04666275]\n",
      " [-0.98354292]]\n",
      "\n",
      "\n",
      "(212, 128, 128, 12)\n",
      "(212, 7)\n",
      "\n",
      "===== Training on robot sample run #:1 with num timesteps: 212\n",
      "\n",
      "\n",
      "Loaded data for timestep 163 in 0.502 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 18 / timestep: 163\n",
      "Model update time: 5.728 secs\n",
      "Current Model Loss: -1.32008\n",
      "[[-1.32008052]\n",
      " [-1.07197952]\n",
      " [-1.0025382 ]\n",
      " [-0.17649454]\n",
      " [-0.01565411]\n",
      " [ 0.23120141]\n",
      " [-0.80773211]\n",
      " [-1.4494276 ]\n",
      " [ 0.12574628]\n",
      " [ 0.04666275]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 15 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 19 / timestep: 15\n",
      "Model update time: 5.711 secs\n",
      "Current Model Loss: -1.6669\n",
      "[[-1.66689682]\n",
      " [-1.32008052]\n",
      " [-1.07197952]\n",
      " [-1.0025382 ]\n",
      " [-0.17649454]\n",
      " [-0.01565411]\n",
      " [ 0.23120141]\n",
      " [-0.80773211]\n",
      " [-1.4494276 ]\n",
      " [ 0.12574628]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 27 in 0.5 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 20 / timestep: 27\n",
      "Model update time: 5.387 secs\n",
      "Current Model Loss: -0.804886\n",
      "[[-0.80488622]\n",
      " [-1.66689682]\n",
      " [-1.32008052]\n",
      " [-1.07197952]\n",
      " [-1.0025382 ]\n",
      " [-0.17649454]\n",
      " [-0.01565411]\n",
      " [ 0.23120141]\n",
      " [-0.80773211]\n",
      " [-1.4494276 ]]\n",
      "\n",
      "\n",
      "Image compare checkpoint...\n",
      "Model prediction time: 1.333 secs for 10 future frames.\n",
      "Wrote new image sample checkpoint at: sample_14905733299339793.png\n",
      "\n",
      "Loaded data for timestep 51 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 21 / timestep: 51\n",
      "Model update time: 6.445 secs\n",
      "Current Model Loss: -0.360958\n",
      "[[-0.36095825]\n",
      " [-0.80488622]\n",
      " [-1.66689682]\n",
      " [-1.32008052]\n",
      " [-1.07197952]\n",
      " [-1.0025382 ]\n",
      " [-0.17649454]\n",
      " [-0.01565411]\n",
      " [ 0.23120141]\n",
      " [-0.80773211]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 20 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 22 / timestep: 20\n",
      "Model update time: 5.87 secs\n",
      "Current Model Loss: -1.21889\n",
      "[[-1.21888757]\n",
      " [-0.36095825]\n",
      " [-0.80488622]\n",
      " [-1.66689682]\n",
      " [-1.32008052]\n",
      " [-1.07197952]\n",
      " [-1.0025382 ]\n",
      " [-0.17649454]\n",
      " [-0.01565411]\n",
      " [ 0.23120141]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 175 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 23 / timestep: 175\n",
      "Model update time: 5.333 secs\n",
      "Current Model Loss: -1.0925\n",
      "[[-1.09250402]\n",
      " [-1.21888757]\n",
      " [-0.36095825]\n",
      " [-0.80488622]\n",
      " [-1.66689682]\n",
      " [-1.32008052]\n",
      " [-1.07197952]\n",
      " [-1.0025382 ]\n",
      " [-0.17649454]\n",
      " [-0.01565411]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 102 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 24 / timestep: 102\n",
      "Model update time: 6.102 secs\n",
      "Current Model Loss: -1.12925\n",
      "[[-1.12924528]\n",
      " [-1.09250402]\n",
      " [-1.21888757]\n",
      " [-0.36095825]\n",
      " [-0.80488622]\n",
      " [-1.66689682]\n",
      " [-1.32008052]\n",
      " [-1.07197952]\n",
      " [-1.0025382 ]\n",
      " [-0.17649454]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 32 in 0.501 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 25 / timestep: 32\n",
      "Model update time: 5.631 secs\n",
      "Current Model Loss: -0.687112\n",
      "[[-0.68711168]\n",
      " [-1.12924528]\n",
      " [-1.09250402]\n",
      " [-1.21888757]\n",
      " [-0.36095825]\n",
      " [-0.80488622]\n",
      " [-1.66689682]\n",
      " [-1.32008052]\n",
      " [-1.07197952]\n",
      " [-1.0025382 ]]\n",
      "\n",
      "\n",
      "Image compare checkpoint...\n",
      "Model prediction time: 0.918 secs for 10 future frames.\n",
      "Wrote new image sample checkpoint at: sample_1490573361497674.png\n",
      "\n",
      "Loaded data for timestep 100 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 26 / timestep: 100\n",
      "Model update time: 5.034 secs\n",
      "Current Model Loss: -1.34298\n",
      "[[-1.34297657]\n",
      " [-0.68711168]\n",
      " [-1.12924528]\n",
      " [-1.09250402]\n",
      " [-1.21888757]\n",
      " [-0.36095825]\n",
      " [-0.80488622]\n",
      " [-1.66689682]\n",
      " [-1.32008052]\n",
      " [-1.07197952]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 53 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 27 / timestep: 53\n",
      "Model update time: 5.146 secs\n",
      "Current Model Loss: -0.488012\n",
      "[[-0.48801163]\n",
      " [-1.34297657]\n",
      " [-0.68711168]\n",
      " [-1.12924528]\n",
      " [-1.09250402]\n",
      " [-1.21888757]\n",
      " [-0.36095825]\n",
      " [-0.80488622]\n",
      " [-1.66689682]\n",
      " [-1.32008052]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 69 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 28 / timestep: 69\n",
      "Model update time: 5.162 secs\n",
      "Current Model Loss: -0.435615\n",
      "[[-0.43561494]\n",
      " [-0.48801163]\n",
      " [-1.34297657]\n",
      " [-0.68711168]\n",
      " [-1.12924528]\n",
      " [-1.09250402]\n",
      " [-1.21888757]\n",
      " [-0.36095825]\n",
      " [-0.80488622]\n",
      " [-1.66689682]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 26 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 29 / timestep: 26\n",
      "Model update time: 5.074 secs\n",
      "Current Model Loss: -0.825751\n",
      "[[-0.82575053]\n",
      " [-0.43561494]\n",
      " [-0.48801163]\n",
      " [-1.34297657]\n",
      " [-0.68711168]\n",
      " [-1.12924528]\n",
      " [-1.09250402]\n",
      " [-1.21888757]\n",
      " [-0.36095825]\n",
      " [-0.80488622]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 170 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 30 / timestep: 170\n",
      "Model update time: 4.975 secs\n",
      "Current Model Loss: -1.71378\n",
      "[[-1.71377969]\n",
      " [-0.82575053]\n",
      " [-0.43561494]\n",
      " [-0.48801163]\n",
      " [-1.34297657]\n",
      " [-0.68711168]\n",
      " [-1.12924528]\n",
      " [-1.09250402]\n",
      " [-1.21888757]\n",
      " [-0.36095825]]\n",
      "\n",
      "\n",
      "Image compare checkpoint...\n",
      "Model prediction time: 0.909 secs for 10 future frames.\n",
      "Wrote new image sample checkpoint at: sample_14905733883560605.png\n",
      "\n",
      "Loaded data for timestep 163 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 31 / timestep: 163\n",
      "Model update time: 5.057 secs\n",
      "Current Model Loss: -1.55279\n",
      "[[-1.55279315]\n",
      " [-1.71377969]\n",
      " [-0.82575053]\n",
      " [-0.43561494]\n",
      " [-0.48801163]\n",
      " [-1.34297657]\n",
      " [-0.68711168]\n",
      " [-1.12924528]\n",
      " [-1.09250402]\n",
      " [-1.21888757]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 6 in 25.038 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 32 / timestep: 6\n",
      "Model update time: 5.009 secs\n",
      "Current Model Loss: -1.62886\n",
      "[[-1.62885928]\n",
      " [-1.55279315]\n",
      " [-1.71377969]\n",
      " [-0.82575053]\n",
      " [-0.43561494]\n",
      " [-0.48801163]\n",
      " [-1.34297657]\n",
      " [-0.68711168]\n",
      " [-1.12924528]\n",
      " [-1.09250402]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 56 in 3.006 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 33 / timestep: 56\n",
      "Model update time: 4.936 secs\n",
      "Current Model Loss: -0.416329\n",
      "[[-0.41632915]\n",
      " [-1.62885928]\n",
      " [-1.55279315]\n",
      " [-1.71377969]\n",
      " [-0.82575053]\n",
      " [-0.43561494]\n",
      " [-0.48801163]\n",
      " [-1.34297657]\n",
      " [-0.68711168]\n",
      " [-1.12924528]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 82 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 34 / timestep: 82\n",
      "Model update time: 4.992 secs\n",
      "Current Model Loss: -0.556532\n",
      "[[-0.55653244]\n",
      " [-0.41632915]\n",
      " [-1.62885928]\n",
      " [-1.55279315]\n",
      " [-1.71377969]\n",
      " [-0.82575053]\n",
      " [-0.43561494]\n",
      " [-0.48801163]\n",
      " [-1.34297657]\n",
      " [-0.68711168]]\n",
      "\n",
      "\n",
      "\n",
      "Loaded data for timestep 101 in 0.0 msecs\n",
      "\n",
      "===================================================\n",
      "iter: 35 / timestep: 101\n",
      "Model update time: 5.007 secs\n",
      "Current Model Loss: -1.2817\n",
      "[[-1.28170347]\n",
      " [-0.55653244]\n",
      " [-0.41632915]\n",
      " [-1.62885928]\n",
      " [-1.55279315]\n",
      " [-1.71377969]\n",
      " [-0.82575053]\n",
      " [-0.43561494]\n",
      " [-0.48801163]\n",
      " [-1.34297657]]\n",
      "\n",
      "\n",
      "Image compare checkpoint...\n"
     ]
    }
   ],
   "source": [
    "########################################## START THE TRAINING #######################################\n",
    "\n",
    "print(\"Running program...\")\n",
    "\n",
    "DEMONSTRATION_FOLDERS = Video_Utils.GetFoldersForRuns()\n",
    "\n",
    "NUM_DEMONSTRATIONS = len(DEMONSTRATION_FOLDERS)\n",
    "\n",
    "for f in DEMONSTRATION_FOLDERS:\n",
    "    print(f)\n",
    "\n",
    "# Separate into training and testing data based on the number of recordings (folders) we have: \n",
    "random.shuffle(DEMONSTRATION_FOLDERS)    \n",
    "lim_separate = round(NUM_DEMONSTRATIONS * PERCENT_TRAIN)    \n",
    "TRAINING_FOLDERS = DEMONSTRATION_FOLDERS[0:lim_separate]\n",
    "TESTING_FOLDERS = DEMONSTRATION_FOLDERS[lim_separate:]\n",
    "NUM_TRAINING_DEMONSTRATIONS = len(TRAINING_FOLDERS)\n",
    "NUM_TESTING_DEMONSTRATIONS = len(TESTING_FOLDERS)\n",
    "\n",
    "print(NUM_TRAINING_DEMONSTRATIONS); print(NUM_TESTING_DEMONSTRATIONS); print(NUM_DEMONSTRATIONS) # a + b = c\n",
    "\n",
    "    \n",
    "\n",
    "# Data flow process: we train on entire folder (sample run of a robot) before moving on to the next to \n",
    "# amortize the time required to load that folder's training data into RAM (multiple seconds). For a dynamics\n",
    "# model this should be perfectly acceptable because the dynamics to be learned are ideally the *same* between\n",
    "# different sample runs recorded on the (simulated) robot. \n",
    "itrs = 0\n",
    "while itrs < TOTAL_TRAINING_ITRS:\n",
    "    \n",
    "    # Choose which expert demonstration we're using: \n",
    "    CURR_DEMONSTRATION = randint(0,NUM_TRAINING_DEMONSTRATIONS-1)\n",
    "\n",
    "    frames, actions = Video_Utils.LoadFramesActionsFromFolder(TRAINING_FOLDERS[CURR_DEMONSTRATION], CAM_W, CAM_H, CAM_C*NUM_CAMS, ACTION_LEN)\n",
    "    # ^ about 5 secs\n",
    "    \n",
    "    print(frames.shape)\n",
    "    print(actions.shape)\n",
    "    \n",
    "    LENGTH_CURR_DEMONSTRATION = frames.shape[0] # number of timesteps for this demonstration\n",
    "    \n",
    "    # New demonstration, reset the relevant data buffers:\n",
    "    PREV_FRAMES_BUFFER = np.zeros((NUM_PAST_FRAMES, CAM_W, CAM_H, CAM_C*NUM_CAMS))\n",
    "    PREV_ACTION_BUFFER = np.zeros((NUM_PAST_FRAMES, ACTION_LEN))\n",
    "    FUTURE_FRAMES_BUFFER = np.zeros((NUM_FUTURE_FRAMES, CAM_W, CAM_H, CAM_C*NUM_CAMS))\n",
    "    FUTURE_ACTION_BUFFER = np.zeros((NUM_FUTURE_FRAMES-1, ACTION_LEN))\n",
    "    \n",
    "    print(\"\\n===== Training on robot sample run #:\"+str(CURR_DEMONSTRATION)+\" with num timesteps: \"+str(LENGTH_CURR_DEMONSTRATION)+\"\\n\")\n",
    "    \n",
    "    for i in range(0, round(LENGTH_CURR_DEMONSTRATION/NUM_FUTURE_FRAMES)): \n",
    "        # only sample a limited number from each demo before moving on to next demo. \n",
    "        t = randint(0,LENGTH_CURR_DEMONSTRATION-1)\t# now we have random shuffle training within a demo.\n",
    "            \n",
    "        ##### STEP 1: Load Buffers with Data #####    \n",
    "        ms1 = time.time()*1000.0 \n",
    "        # Past data: (up to and including x(t), and u(t) - which causes x(t+1), etc)\n",
    "        if (t >= NUM_PAST_FRAMES): # regular, in-bounds\n",
    "            PREV_FRAMES_BUFFER = frames[t-NUM_PAST_FRAMES:t, :] \n",
    "            PREV_ACTION_BUFFER_BUFFER = actions[t-NUM_PAST_FRAMES:t, :]\n",
    "        else: # t is less than past frames, need to concat zeros at beginning\n",
    "            LIM1 = np.abs(t - NUM_PAST_FRAMES) \n",
    "            PREV_FRAMES_BUFFER = np.concatenate((np.zeros((LIM1, CAM_W, CAM_H, CAM_C*NUM_CAMS)), frames[0:t, :]), axis=0) \n",
    "            PREV_ACTIONS_BUFFER = np.concatenate((np.zeros((LIM1, ACTION_LEN)), actions[0:t, :]), axis=0)\n",
    "        # Future Data: (include 1 less future action than frame to retain logical p(x(t+1)|x(t),u(t)) structure)\n",
    "        if (t <= LENGTH_CURR_DEMONSTRATION - NUM_FUTURE_FRAMES - 1):\n",
    "            FUTURE_FRAMES_BUFFER = frames[t:t+NUM_FUTURE_FRAMES, :]\n",
    "            FUTURE_ACTIONS_BUFFER = actions[t:t+NUM_FUTURE_FRAMES-1, :]\n",
    "        else: # running off the end\n",
    "            LIM2 = t + NUM_FUTURE_FRAMES - LENGTH_CURR_DEMONSTRATION \n",
    "            # ^ We need to repeat last frame this many times / add this many zero actions\n",
    "            FUTURE_FRAMES_BUFFER = frames[t:LENGTH_CURR_DEMONSTRATION, :]\n",
    "            last_frame = np.expand_dims(frames[LENGTH_CURR_DEMONSTRATION-1], axis=0)\n",
    "            for j in range(0,LIM2):\n",
    "                FUTURE_FRAMES_BUFFER = np.concatenate((FUTURE_FRAMES_BUFFER, last_frame), axis=0) \n",
    "            FUTURE_ACTIONS_BUFFER = np.concatenate((actions[t:LENGTH_CURR_DEMONSTRATION, :], np.zeros((LIM2, ACTION_LEN))), axis=0)\n",
    "        ms2 = time.time()*1000.0\n",
    "        print(\"\\nLoaded data for timestep \"+str(t)+\" in \"+str(round((ms2-ms1)/1, 3))+\" msecs\\n\")\n",
    "        \n",
    "        #print(PREV_FRAMES_BUFFER.shape)\n",
    "        #print(PREV_ACTION_BUFFER.shape)\n",
    "        #print(FUTURE_FRAMES_BUFFER.shape)\n",
    "        #print(FUTURE_ACTION_BUFFER.shape)\n",
    "        \n",
    "        ##### STEP 2: Train on Batch of Data Gathered Above #####\n",
    "        ms1 = time.time()*1000.0\n",
    "        # inputs: [input_prev_frames_raw, input_prev_actions_raw, input_future_actions_raw]\n",
    "        # outputs/training targets: [future_frames]\n",
    "        model_loss = UDM_model.train_on_batch(\n",
    "                     [np.expand_dims(PREV_FRAMES_BUFFER.reshape(CAM_W, CAM_H, CAM_C*NUM_CAMS*NUM_PAST_FRAMES), axis=0), \n",
    "                      np.expand_dims(PREV_ACTION_BUFFER.reshape(-1), axis=0), \n",
    "                      np.expand_dims(FUTURE_ACTION_BUFFER.reshape(-1), axis=0)], # <--- inputs \n",
    "                     [np.expand_dims(FUTURE_FRAMES_BUFFER.reshape(CAM_W, CAM_H, CAM_C*NUM_CAMS*NUM_PAST_FRAMES), axis=0)])\n",
    "        \n",
    "        ms2 = time.time()*1000.0\n",
    "        print(\"===================================================\")\n",
    "        print(\"iter: \" + str(itrs) + \" / timestep: \"+str(t))\n",
    "        print(\"Model update time: \"+str(round((ms2-ms1)/1000, 3)) + ' secs')\n",
    "        print(\"Current Model Loss: \"+str(model_loss))\n",
    "        MODEL_LOSS_BUFFER = np.roll(MODEL_LOSS_BUFFER, 1); MODEL_LOSS_BUFFER[0] = model_loss\n",
    "        print(MODEL_LOSS_BUFFER); print(\"\\n\")\n",
    "        \n",
    "        ##### STEP 3: CHECKPOINTS #####\n",
    "        \n",
    "        if (itrs % IMAGE_COMPARE_CHECKPOINT == 0 ): # View how the model is doing\n",
    "            print(\"Image compare checkpoint...\")\n",
    "            timestamp = str(time.time()).replace(\".\",\"\")\n",
    "            \n",
    "            # Get the generated future frames: \n",
    "            ms1 = time.time()*1000.0\n",
    "            GEN_FUTURE_FRAMES = UDM_model.predict(\n",
    "                                [np.expand_dims(PREV_FRAMES_BUFFER.reshape(CAM_W, CAM_H, CAM_C*NUM_CAMS*NUM_PAST_FRAMES), axis=0), \n",
    "                                 np.expand_dims(PREV_ACTION_BUFFER.reshape(-1), axis=0), \n",
    "                                 np.expand_dims(FUTURE_ACTION_BUFFER.reshape(-1), axis=0)])\n",
    "            GEN_FUTURE_FRAMES = GEN_FUTURE_FRAMES[0]\n",
    "            ms2 = time.time()*1000.0\n",
    "            print(\"Model prediction time: \"+str(round((ms2-ms1)/1000, 3)) +' secs'+' for '+str(NUM_FUTURE_FRAMES)+' future frames.')\n",
    "            \n",
    "            imgs_gen = Video_Utils.ViewFutureFrames(GEN_FUTURE_FRAMES.reshape(NUM_FUTURE_FRAMES, CAM_W, CAM_H, CAM_C*NUM_CAMS))\n",
    "            imgs_split = np.zeros((30, NUM_FUTURE_FRAMES*CAM_W, 3)) # goes in between to separate real/generated images\n",
    "            imgs_gt = Video_Utils.ViewFutureFrames(FUTURE_FRAMES_BUFFER)\n",
    "            #print(imgs_gt.shape); print(imgs_split.shape); print(imgs_gen.shape);\n",
    "            imgs_compare = np.concatenate((imgs_gt, imgs_split, imgs_gen),axis=0)\n",
    "            #print(imgs_compare.shape)\n",
    "            #cv2.imshow('image',imgs_compare) \n",
    "            #cv2.waitKey(0)\n",
    "            img_filename = 'sample_'+timestamp+'.png' \n",
    "            cv2.imwrite(img_filename,(imgs_compare*255 + 127.5))\n",
    "            print(\"Wrote new image sample checkpoint at: \"+img_filename)\n",
    "            \n",
    "        if (itrs % SAVE_CHECKPOINT_ITRS == 0): # save progress\n",
    "            print(\"Model save checkpoint, itr: \"+str(itrs))\n",
    "            ms1 = time.time()*1000.0\n",
    "            timestamp = str(time.time()).replace(\".\",\"\")\n",
    "            mean_recent_loss = round(np.mean(MODEL_LOSS_BUFFER), 4)\n",
    "            model_str_name = 'UDM_weights_'+str(mean_recent_loss)+'.h5' # pro-tip: manually re-name after each run... \n",
    "            UDM_model.save(model_str_name)\n",
    "            ms2 = time.time()*1000.0\n",
    "            print(\"Model save time: \"+str(round((ms2-ms1)/1000, 3)) +' secs')\n",
    "\n",
    "        itrs = itrs + 1 # don't forget to increment total training itrs counter    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
